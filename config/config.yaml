train:
  device: cuda
  batch_size: 4
  dtype: bfloat16
  learning_rate: 2e-4
  num_epochs: 10
  warmup_iters: 0
  grad_accumulation_steps: 1
  max_grad_norm: 1.0
  log_interval: 10
  validation_interval: 1000
  wandb_project: "discrete_diffusion"
  use_wandb: True


  
model:
  vocab_size: 28
  max_seq_len: 512
  dim: 256
  num_heads: 4
  num_transformer_layers: 6
  dropout: 0.1
  flash_attn: False
  rms_norm_eps: 1e-6
  ffn_multiple: 4
  compile_model: False