train:
  device: cuda
  batch_size: 4
  dtype: bfloat16
  learning_rate: 1e-5
  num_epochs: 10
  warmup_iters: 0
  grad_accumulation_steps: 1
  max_grad_norm: 1.0
  log_interval: 10
  validation_interval: 10000
  wandb_project: "discrete_diffusion"
  use_wandb: True
  overfit_mode: False


  
model:
  vocab_size: 28
  max_seq_len: 256
  dim: 512
  num_heads: 8
  num_transformer_layers: 16
  dropout: 0.1
  flash_attn: False
  rms_norm_eps: 1e-6
  ffn_multiple: 4
  compile_model: False